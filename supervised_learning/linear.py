import numpy as np
from sklearn.metrics import accuracy_score

class LinearRegressionSML():
  
  """ 
  Implements Linear Regression bases on OLS or Gradient Descent, as chosen by the user. 
  
  """

  def __init__(self,learning_rate=0.01,num_iterations=1000,verbose=False):
    
    self.coefficients = None
    self.intercept = None
    self.learning_rate = learning_rate
    self.num_iterations = num_iterations
    self.verbose = verbose

  def fit(self,X,y,method="ols",learning_rate = None,num_iterations = None,lambda_val = None,verbose=None):
    """ It fits the coefficientes w = (w1,...,wn) to the data in order to minimize the residual sum of squares between the data and the linear model.
    Two approaches are supported: "ols" or "gradientdesc"

    Args:
        X (numpy.ndarray): 
            Training data. Shape ``(n_samples,n_features)``
        y (numpy.ndarray):
            Target training data. Shape ``(n_samples,)``
        method (str):
            Method used to estimate the coefficients of the linear model.
        learing_rate (float):
            The learning rate used to modify the parameters.
        num_iterations (int):
            Number of iterations for the application of gradient descent.
        lambda_val (float):
            Lambda coefficient for applying regularization.
        verbose (bool):
            Wether to display iteration information during training.

    Returns:
        self (object):
            Fitted model

    """
    if verbose is not None:
      self.verbose=verbose
    if (method == "ols"):
      self.fit_ols(X,y)
    elif (method =="gradientdesc"):
      self.fit_gd(X,y,learning_rate=learning_rate,num_iterations=num_iterations)
    else:
      raise ValueError("Invalid fitting method. Please choose 'ols' or 'gradientdesc'.")

  def fit_ols(self,X,y):
    # Add a column of ones for the intercept term
    ones = np.ones((X.shape[0],1))
    X = np.hstack((ones, X))

    # Calculate coefficients using the Ordinary Least Squares formula: B = X'X^(-1)X'y
    self.coefficients = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),y)

  def fit_gd(self,X,y,learning_rate,num_iterations):

    # Get the number of samples and features
    n_samples, n_features = X.shape

    # Add a column of ones for the intercept term
    ones = np.ones((n_samples,1))
    X = np.hstack((ones, X))

    # Initialize the coefficients to zeros
    self.intercept = 0.0
    self.coefficients = np.ones(n_features+1)

    # For each iteration, generate predictions, calculate the error and update the coefficients according to the learning rate and the gradient of the loss.
    for iter in range(num_iterations):

      # Generate predictions
      y_pred = np.dot(X,self.coefficients)

      # Calculate the error
      error = y - y_pred

      # Update intercept and coefficients
      self.coefficients += (learning_rate * np.dot(X.T,error))
      
      if self.verbose:
        if (iter + 1) % 10 == 0:
          mse = self.evaluate(X[:,1:],y)
          print(f"Iteration {iter + 1}: Mean Squared Error = {mse:.2f}")

  def predict(self,X):
    """
    Predict.

    Predicts the target values with the estimated linear model.

    Parameters
    ----------
    X : numpy.ndarray. Shape (n_samples,n_features)
        Data to generate the prediction on.

    Returns
    -------
    y_pred : numpy.ndarray. Shape (n_samples,1)
             Generated predictions

    """
    # Add a column of ones for the intercept term
    ones = np.ones((X.shape[0],1))
    X = np.hstack((ones, X))

    # Generate predictions with y_pred = Bx
    y_pred = np.dot(X,self.coefficients)

    return y_pred

  def get_coefficients(self):
    """
    Get coefficients.

    Returns the estimated coefficients.

    Returns
    -------
    coefficients: numpy.ndarray. Shape(num_features + 1,1)
                  Estimated coefficients and intercept.
    """
    coefficients = np.concatenate(([self.intercept],self.coefficients))
    return coefficients

  def evaluate(self,X,y):
    """
    Evaluate.

    Evaluate the predictions generated by the fitted model.

    Parameters
    ----------
    X : numpy.ndarray. Shape (n_samples,n_features)
        Test data.

    y : numpy.ndarray. Shape (n_samples,)
        Target test data.

    Returns
    -------
    metric : float.
             Calculated metric.
    """
    # Generate predictions on test data
    pred = self.predict(X)
    mse = np.mean((y - pred)**2)

    return mse
  
class LogisticRegressionSML():
  """
  A classed used to represent a Logistic Regression model.

  """

  def __init__(self,lr=0.001,num_iter=1000,verbose=True):
    #initial parameters
    self.coefficients = None
    self.intercept = None
    self.lr = lr
    self.verbose = verbose
    self.num_iter = num_iter
    
  def fit(self, X, y, method="gradientdesc",lr=None,num_iter=None):
    """
    Fits a Logistic regression model's coefficients to the data X.
    Two methods are supported: "gradientdesc" or "stochasticgradientdesc"

    Parameters
    ----------
    X : numpy.ndarray
        Training examples
    y : numpy.ndarray
        Training target values
    method : str
        Method to fit the Logistic regression model
      
    Raises
    ------
    ValueError
      If an invalid method is provided
    """

    if method == 'gradientdesc':
      self.fit_gd(X,y,lr,num_iter)
    elif method == 'stochasticgradientdesc':
      self.fit_sgd(X,y,lr,num_iter)
    else:
      raise ValueError("Invalid fitting method. Please choose between: gradientdesc and stochasticgradientdesc")
    
  def fit_gd(self,X,y,lr,num_iter):
    """
    Fits the logistic regression model using Gradient Descent.
    """
    lr = lr if lr else self.lr
    num_iter = num_iter if num_iter else self.num_iter

    # Get the number of samples and features
    n_samples, n_features = X.shape

    # Add ones to represent the intercept term in X
    ones = np.ones((n_samples,1))
    X = np.hstack((ones, X))

    # Initialize coefficients to 1
    self.coefficients = np.ones(n_features+1)

    # Update parameters for each iteration
    for iter in range(num_iter):
      # Calculate predicted values
      z_preds = np.dot(X,self.coefficients)
      y_preds = self.sigmoid(z_preds)
        
      # Calculate the error for all samples
      error = y - y_preds
        
      # Update coefficients
      self.coefficients += (lr * np.dot(X.T,error))

      if self.verbose:
        if (iter + 1) % 10 == 0:
          loss = self.compute_loss(y, y_preds)
          acc = self.evaluate(X[:,1:],y)
          print(f"Iteration {iter + 1}: Accuracy = {acc:.3f}; Loss = {loss:.5f}")
    
  def fit_sgd(self,X,y,lr=None,num_iter=None):
    """
    Fits the logistic regression model using Stochastic Gradient Descent
    """
    lr = lr if lr is not None else self.lr
    num_iter = num_iter if num_iter is not None else self.num_iter

    n_samples, n_features = X.shape
    
    ones = np.ones((n_samples,1))
    X = np.hstack((ones,X))

    self.coefficients = np.ones(n_features+1)

    for epoch in range(num_iter):
        
      # Shuffle the data at the beginning of each epoch
      indices = np.random.permutation(n_samples)

      for idx in indices:
        xi, yi = X[idx], y[idx]
        z_pred = np.dot(xi,self.coefficients)
        y_pred = self.sigmoid(z_pred)

        error = yi - y_pred

        self.coefficients += lr * xi * error

        #lr *= 0.99  # Example: decay the learning rate by 1% each epoch

      if self.verbose and (epoch + 1) % 10 == 0:  # Change logging frequency if needed
        loss = self.compute_loss(y, self.predict(X[:, 1:]))
        acc = self.evaluate(X[:, 1:], y)
        print(f"Epoch {epoch + 1}: Loss = {loss:.5f}, Accuracy = {acc:.3f}")

                              
  def predict(self,X):
    """
    Generates prediction for the given data
    """
    # Add ones to X for the intercept term
    n_samples = X.shape[0]
    ones = np.ones((n_samples,1))
    X = np.hstack((ones,X))

    # Generate predictions
    z_preds = np.dot(X, self.coefficients)
    y_preds = self.sigmoid(z_preds)

    # Define a threshold (e.g., 0.5)
    threshold = 0.5

    # Apply the threshold to consider y_pred as 1 or 0
    y_pred_binary = (y_preds > threshold).astype(int)
      
    return y_pred_binary
    
  def compute_loss(self, y_true, y_pred):
    epsilon = 1e-15  # small value to ensure log(0) does not cause issues
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    return loss

  def evaluate(self,X,y):
    """
    Evaluate the fitted model.
    """
    y_pred = self.predict(X)

    # Define a threshold (e.g., 0.5)
    threshold = 0.5

    # Apply the threshold to consider y_pred as 1 or 0
    y_pred_binary = (y_pred > threshold).astype(int)
    acc = accuracy_score(y,y_pred_binary)

    return acc


  def sigmoid(self,x):
    """
    Computes the logistic function.

    Parameters
    ----------
    x : float or numpy.ndarray
        Input value or array of values

    Returns
    -------
    float or numpy.ndarray
        Computed logistic function value or array of values
    """
    x = np.clip(x, -250, 250)
    return 1 / (1 + np.exp(-x))
